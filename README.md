### 적대적 공격이란?

적대적 공격(Adversarial Attack)은 기계 학습 모델을 속이기 위해 설계된 입력 데이터의 수정입니다. 이러한 수정은 사람이나 다른 기계 학습 모델에는 거의 눈에 띄지 않지만, 목표 모델에 대해 잘못된 예측을 유도하거나 원하는 출력을 얻기 위해 조작됩니다.

### FGSM (Fast Gradient Sign Method)

FGSM은 적대적 공격의 간단하면서도 효과적인 기법 중 하나입니다. 다음과 같은 단계로 작동합니다:

1. **기울기 계산:** 목표 모델에서 입력 이미지의 손실 함수를 통해 기울기를 계산합니다.
   
2. **기울기의 부호 활용:** 입력 이미지의 각 픽셀에 대해 기울기의 부호를 이용하여 방향을 설정합니다. 목표는 손실을 최대화하도록 이미지를 조정하는 것입니다.
   
3. **입력 이미지 조정:** 설정된 방향으로 이미지를 작은 양의 엡실론(epsilon) 값으로 조정하여 적대적 예제를 생성합니다.
   
4. **클리핑:** 생성된 적대적 예제는 픽셀 값이 허용 범위를 초과하지 않도록 클리핑하여 이미지의 자연스러움을 유지합니다.



### 예시

  ![Adversarial Image](https://github.com/lee-seong-wook/Adversarial_Attack/assets/130055880/75be9070-7eee-4650-82ea-a2589e9e542d)

### 참고 사항

- `epsilon` 값을 조정하여 왜곡의 크기를 제어할 수 있습니다. 높은 값은 적대적 이미지에서 더 큰 변화를 초래할 수 있습니다.


더 많은 모델, 데이터셋 또는 공격 전략을 실험하고 수정하는 것에 자유롭게 도전해 보세요. FGSM 기법과 그 영향에 대한 자세한 내용은 [Goodfellow et al., 2014](https://arxiv.org/abs/1412.6572)를 참조하시길 바랍니다.






